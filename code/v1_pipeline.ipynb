{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU Patent Data Pipeline v1: Core, Expansion, and ExpansionXVocab Datasets\n",
    "\n",
    "This notebook implements a three-dataset pipeline for patent analysis:\n",
    "\n",
    "## Dataset Definitions\n",
    "\n",
    "1. **Core Dataset**: Patents with CPC codes related to:\n",
    "   - G06F 9/3887, G06F 9/3888, G06F 9/38885 (Parallel processing)\n",
    "   - G06F 9/3009 (Multiprocessing arrangements)\n",
    "   - G06F 12/0842, G06F 12/0844 (Cache memory)\n",
    "   - G06F 13/42, G06F 13/14, G06F 13/16 (Bus architectures)\n",
    "\n",
    "2. **Expansion Dataset**: Patents with CPC codes:\n",
    "   - G06F 15/8007, G06F 15/8053 (Multiprocessor systems)\n",
    "   - G06N 3/06 (Neural networks)\n",
    "\n",
    "3. **ExpansionXVocab Dataset**: Patents from Expansion that contain keywords:\n",
    "   - gpu\n",
    "   - high-performance compute\n",
    "   - hpc\n",
    "\n",
    "## Setup\n",
    "1. Install required packages: `pip install requests pandas pyarrow`\n",
    "2. Set your lens.org API token in environment variable `LENS_API_TOKEN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import gzip\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Configuration\n",
    "API_TOKEN = os.getenv('LENS_API_TOKEN', 'your-api-token-here')\n",
    "API_URL = 'https://api.lens.org/patent/search'\n",
    "\n",
    "# Base data path\n",
    "BASE_PATH = Path('../data/patents/v1_core_expansion')\n",
    "\n",
    "# Dataset-specific CPC codes\n",
    "CORE_CPC_CODES = [\n",
    "    'G06F9/3887',   # Parallel processing\n",
    "    'G06F9/3888',   # Parallel processing\n",
    "    'G06F9/38885',  # Parallel processing\n",
    "    'G06F9/3009',   # Multiprocessing arrangements\n",
    "    'G06F12/0842',  # Cache memory\n",
    "    'G06F12/0844',  # Cache memory\n",
    "    'G06F13/42',    # Bus architectures\n",
    "    'G06F13/14',    # Bus architectures\n",
    "    'G06F13/16',    # Bus architectures\n",
    "]\n",
    "\n",
    "EXPANSION_CPC_CODES = [\n",
    "    'G06F15/8007',  # Multiprocessor systems\n",
    "    'G06F15/8053',  # Multiprocessor systems\n",
    "    'G06N3/06',     # Neural networks\n",
    "]\n",
    "\n",
    "# Keywords for ExpansionXVocab filtering\n",
    "VOCAB_KEYWORDS = [\n",
    "    'gpu',\n",
    "    'high-performance compute',\n",
    "    'hpc'\n",
    "]\n",
    "\n",
    "# Search parameters\n",
    "JURISDICTION = 'US'  # United States patents only\n",
    "MAX_RESULTS = 1000   # Max results per query (adjust based on needs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directories(base_path, dataset_name):\n",
    "    \"\"\"Create directory structure for a dataset.\"\"\"\n",
    "    dataset_path = base_path / dataset_name\n",
    "    for subdir in ['raw', 'parsed', 'text_clean', 'embeddings', 'logs']:\n",
    "        (dataset_path / subdir).mkdir(parents=True, exist_ok=True)\n",
    "    return dataset_path\n",
    "\n",
    "def build_cpc_query(cpc_codes, jurisdiction, max_results=1000):\n",
    "    \"\"\"\n",
    "    Build lens.org API query for patents with specific CPC codes.\n",
    "    \n",
    "    Args:\n",
    "        cpc_codes: List of CPC classification codes\n",
    "        jurisdiction: Patent jurisdiction (e.g., 'US')\n",
    "        max_results: Maximum number of results to return\n",
    "    \n",
    "    Returns:\n",
    "        Query dictionary for lens.org API\n",
    "    \"\"\"\n",
    "    query = {\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": [\n",
    "                    {\n",
    "                        \"bool\": {\n",
    "                            \"should\": [\n",
    "                                {\"term\": {\"classification_cpc.classification_id\": code}} \n",
    "                                for code in cpc_codes\n",
    "                            ],\n",
    "                            \"minimum_should_match\": 1\n",
    "                        }\n",
    "                    },\n",
    "                    {\"term\": {\"jurisdiction\": jurisdiction}}\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        \"size\": max_results,\n",
    "        \"include\": [\n",
    "            \"lens_id\",\n",
    "            \"title\",\n",
    "            \"abstract\",\n",
    "            \"description\",\n",
    "            \"claims\",\n",
    "            \"date_published\",\n",
    "            \"jurisdiction\",\n",
    "            \"applicants\",\n",
    "            \"inventors\",\n",
    "            \"classification_cpc\",\n",
    "            \"biblio\"\n",
    "        ]\n",
    "    }\n",
    "    return query\n",
    "\n",
    "def fetch_patents(api_token, query):\n",
    "    \"\"\"\n",
    "    Fetch patents from lens.org API.\n",
    "    \n",
    "    Args:\n",
    "        api_token: lens.org API token\n",
    "        query: Query dictionary\n",
    "    \n",
    "    Returns:\n",
    "        API response data\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {api_token}',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    \n",
    "    response = requests.post(API_URL, json=query, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    return response.json()\n",
    "\n",
    "def save_raw_data(raw_data, output_path, dataset_name, timestamp):\n",
    "    \"\"\"Save raw data as compressed JSON.\"\"\"\n",
    "    raw_file = output_path / 'raw' / f'{dataset_name}_{timestamp}.json.gz'\n",
    "    with gzip.open(raw_file, 'wt', encoding='utf-8') as f:\n",
    "        json.dump(raw_data, f, indent=2)\n",
    "    return raw_file\n",
    "\n",
    "def parse_patents(raw_data):\n",
    "    \"\"\"\n",
    "    Parse raw patent data into normalized schema.\n",
    "    \n",
    "    Args:\n",
    "        raw_data: Raw API response\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with normalized patent data\n",
    "    \"\"\"\n",
    "    patents = raw_data.get('data', [])\n",
    "    \n",
    "    parsed_records = []\n",
    "    for patent in patents:\n",
    "        record = {\n",
    "            'lens_id': patent.get('lens_id'),\n",
    "            'title': patent.get('title', ''),\n",
    "            'abstract': patent.get('abstract', ''),\n",
    "            'description': patent.get('description', ''),\n",
    "            'date_published': patent.get('date_published'),\n",
    "            'jurisdiction': patent.get('jurisdiction'),\n",
    "            'applicants': json.dumps(patent.get('applicants', [])),\n",
    "            'inventors': json.dumps(patent.get('inventors', [])),\n",
    "            'cpc_codes': json.dumps([c.get('classification_id') for c in patent.get('classification_cpc', [])]),\n",
    "            'claims_count': len(patent.get('claims', [])),\n",
    "            'first_claim': patent.get('claims', [{}])[0].get('claim_text') if patent.get('claims') else None\n",
    "        }\n",
    "        parsed_records.append(record)\n",
    "    \n",
    "    return pd.DataFrame(parsed_records)\n",
    "\n",
    "def save_parsed_data(df, output_path, dataset_name, timestamp):\n",
    "    \"\"\"Save parsed data as parquet.\"\"\"\n",
    "    parsed_file = output_path / 'parsed' / f'{dataset_name}_{timestamp}.parquet'\n",
    "    df.to_parquet(parsed_file, index=False)\n",
    "    return parsed_file\n",
    "\n",
    "def contains_keywords(text, keywords):\n",
    "    \"\"\"\n",
    "    Check if text contains any of the specified keywords (case-insensitive).\n",
    "    \n",
    "    Args:\n",
    "        text: Text to search in\n",
    "        keywords: List of keywords to search for\n",
    "    \n",
    "    Returns:\n",
    "        True if any keyword is found, False otherwise\n",
    "    \"\"\"\n",
    "    if not text or pd.isna(text):\n",
    "        return False\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    return any(keyword.lower() in text_lower for keyword in keywords)\n",
    "\n",
    "def filter_by_keywords(df, keywords):\n",
    "    \"\"\"\n",
    "    Filter dataframe to include only patents containing specified keywords.\n",
    "    Searches in title, abstract, and description fields.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with patent data\n",
    "        keywords: List of keywords to filter by\n",
    "    \n",
    "    Returns:\n",
    "        Filtered DataFrame\n",
    "    \"\"\"\n",
    "    mask = df.apply(\n",
    "        lambda row: (\n",
    "            contains_keywords(row['title'], keywords) or\n",
    "            contains_keywords(row['abstract'], keywords) or\n",
    "            contains_keywords(row['description'], keywords)\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    return df[mask].copy()\n",
    "\n",
    "def log_pipeline_run(output_path, dataset_name, timestamp, query, results_count, file_info):\n",
    "    \"\"\"Log pipeline execution details.\"\"\"\n",
    "    log_file = output_path / 'logs' / f'{dataset_name}_{timestamp}.log.json'\n",
    "    log_data = {\n",
    "        'timestamp': timestamp,\n",
    "        'dataset': dataset_name,\n",
    "        'query': query,\n",
    "        'results_count': results_count,\n",
    "        'files': file_info\n",
    "    }\n",
    "    with open(log_file, 'w') as f:\n",
    "        json.dump(log_data, f, indent=2)\n",
    "    return log_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Fetch and Process Core Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CORE DATASET PIPELINE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create directories\n",
    "core_path = create_directories(BASE_PATH, 'core')\n",
    "print(f\"\\nâœ“ Directories created at: {core_path}\")\n",
    "\n",
    "# Build query\n",
    "core_query = build_cpc_query(CORE_CPC_CODES, JURISDICTION, MAX_RESULTS)\n",
    "print(f\"\\nðŸ“‹ Query configuration:\")\n",
    "print(f\"   CPC Codes: {len(CORE_CPC_CODES)} codes\")\n",
    "print(f\"   Jurisdiction: {JURISDICTION}\")\n",
    "print(f\"   Max Results: {MAX_RESULTS}\")\n",
    "\n",
    "# Fetch data\n",
    "print(f\"\\nðŸ” Fetching core patents from lens.org...\")\n",
    "try:\n",
    "    core_raw_data = fetch_patents(API_TOKEN, core_query)\n",
    "    core_count = len(core_raw_data.get('data', []))\n",
    "    print(f\"âœ“ Successfully fetched {core_count} patents\")\n",
    "    print(f\"  Total results available: {core_raw_data.get('total', 0)}\")\n",
    "    \n",
    "    # Save raw data\n",
    "    core_raw_file = save_raw_data(core_raw_data, core_path, 'core', timestamp)\n",
    "    print(f\"\\nâœ“ Raw data saved to: {core_raw_file}\")\n",
    "    print(f\"  File size: {core_raw_file.stat().st_size / 1024:.2f} KB\")\n",
    "    \n",
    "    # Parse and save\n",
    "    df_core = parse_patents(core_raw_data)\n",
    "    core_parsed_file = save_parsed_data(df_core, core_path, 'core', timestamp)\n",
    "    print(f\"\\nâœ“ Parsed data saved to: {core_parsed_file}\")\n",
    "    print(f\"  Records: {len(df_core)}\")\n",
    "    \n",
    "    # Log execution\n",
    "    core_log = log_pipeline_run(\n",
    "        core_path, 'core', timestamp, core_query, core_count,\n",
    "        {'raw': str(core_raw_file), 'parsed': str(core_parsed_file)}\n",
    "    )\n",
    "    print(f\"\\nâœ“ Log saved to: {core_log}\")\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Sample record:\")\n",
    "    if len(df_core) > 0:\n",
    "        print(df_core[['lens_id', 'title']].head(3).to_string(index=False))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âœ— Error: {e}\")\n",
    "    core_raw_data = None\n",
    "    df_core = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fetch and Process Expansion Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPANSION DATASET PIPELINE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create directories\n",
    "expansion_path = create_directories(BASE_PATH, 'expansion')\n",
    "print(f\"\\nâœ“ Directories created at: {expansion_path}\")\n",
    "\n",
    "# Build query\n",
    "expansion_query = build_cpc_query(EXPANSION_CPC_CODES, JURISDICTION, MAX_RESULTS)\n",
    "print(f\"\\nðŸ“‹ Query configuration:\")\n",
    "print(f\"   CPC Codes: {len(EXPANSION_CPC_CODES)} codes\")\n",
    "print(f\"   Jurisdiction: {JURISDICTION}\")\n",
    "print(f\"   Max Results: {MAX_RESULTS}\")\n",
    "\n",
    "# Fetch data\n",
    "print(f\"\\nðŸ” Fetching expansion patents from lens.org...\")\n",
    "try:\n",
    "    expansion_raw_data = fetch_patents(API_TOKEN, expansion_query)\n",
    "    expansion_count = len(expansion_raw_data.get('data', []))\n",
    "    print(f\"âœ“ Successfully fetched {expansion_count} patents\")\n",
    "    print(f\"  Total results available: {expansion_raw_data.get('total', 0)}\")\n",
    "    \n",
    "    # Save raw data\n",
    "    expansion_raw_file = save_raw_data(expansion_raw_data, expansion_path, 'expansion', timestamp)\n",
    "    print(f\"\\nâœ“ Raw data saved to: {expansion_raw_file}\")\n",
    "    print(f\"  File size: {expansion_raw_file.stat().st_size / 1024:.2f} KB\")\n",
    "    \n",
    "    # Parse and save\n",
    "    df_expansion = parse_patents(expansion_raw_data)\n",
    "    expansion_parsed_file = save_parsed_data(df_expansion, expansion_path, 'expansion', timestamp)\n",
    "    print(f\"\\nâœ“ Parsed data saved to: {expansion_parsed_file}\")\n",
    "    print(f\"  Records: {len(df_expansion)}\")\n",
    "    \n",
    "    # Log execution\n",
    "    expansion_log = log_pipeline_run(\n",
    "        expansion_path, 'expansion', timestamp, expansion_query, expansion_count,\n",
    "        {'raw': str(expansion_raw_file), 'parsed': str(expansion_parsed_file)}\n",
    "    )\n",
    "    print(f\"\\nâœ“ Log saved to: {expansion_log}\")\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Sample record:\")\n",
    "    if len(df_expansion) > 0:\n",
    "        print(df_expansion[['lens_id', 'title']].head(3).to_string(index=False))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âœ— Error: {e}\")\n",
    "    expansion_raw_data = None\n",
    "    df_expansion = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Filter and Process ExpansionXVocab Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPANSIONXVOCAB DATASET PIPELINE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if df_expansion is not None:\n",
    "    # Create directories\n",
    "    expansionxvocab_path = create_directories(BASE_PATH, 'expansionxvocab')\n",
    "    print(f\"\\nâœ“ Directories created at: {expansionxvocab_path}\")\n",
    "    \n",
    "    print(f\"\\nðŸ“‹ Filter configuration:\")\n",
    "    print(f\"   Source: Expansion dataset ({len(df_expansion)} patents)\")\n",
    "    print(f\"   Keywords: {', '.join(VOCAB_KEYWORDS)}\")\n",
    "    \n",
    "    # Filter by keywords\n",
    "    print(f\"\\nðŸ” Filtering expansion patents by keywords...\")\n",
    "    df_expansionxvocab = filter_by_keywords(df_expansion, VOCAB_KEYWORDS)\n",
    "    vocab_count = len(df_expansionxvocab)\n",
    "    print(f\"âœ“ Filtered to {vocab_count} patents ({vocab_count/len(df_expansion)*100:.1f}% of expansion)\")\n",
    "    \n",
    "    # Save filtered data (we use the expansion raw data as source)\n",
    "    if vocab_count > 0:\n",
    "        # Create a filtered raw dataset\n",
    "        filtered_raw_data = {\n",
    "            'total': vocab_count,\n",
    "            'data': [\n",
    "                patent for patent in expansion_raw_data.get('data', [])\n",
    "                if patent.get('lens_id') in df_expansionxvocab['lens_id'].values\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        vocab_raw_file = save_raw_data(filtered_raw_data, expansionxvocab_path, 'expansionxvocab', timestamp)\n",
    "        print(f\"\\nâœ“ Raw data saved to: {vocab_raw_file}\")\n",
    "        print(f\"  File size: {vocab_raw_file.stat().st_size / 1024:.2f} KB\")\n",
    "        \n",
    "        # Save parsed data\n",
    "        vocab_parsed_file = save_parsed_data(df_expansionxvocab, expansionxvocab_path, 'expansionxvocab', timestamp)\n",
    "        print(f\"\\nâœ“ Parsed data saved to: {vocab_parsed_file}\")\n",
    "        print(f\"  Records: {len(df_expansionxvocab)}\")\n",
    "        \n",
    "        # Log execution\n",
    "        vocab_log = log_pipeline_run(\n",
    "            expansionxvocab_path, 'expansionxvocab', timestamp,\n",
    "            {'source': 'expansion', 'keywords': VOCAB_KEYWORDS}, vocab_count,\n",
    "            {'raw': str(vocab_raw_file), 'parsed': str(vocab_parsed_file)}\n",
    "        )\n",
    "        print(f\"\\nâœ“ Log saved to: {vocab_log}\")\n",
    "        \n",
    "        print(f\"\\nðŸ“Š Sample record:\")\n",
    "        print(df_expansionxvocab[['lens_id', 'title']].head(3).to_string(index=False))\n",
    "    else:\n",
    "        print(\"\\nâš  No patents matched the keyword filter\")\n",
    "else:\n",
    "    print(\"\\nâœ— Cannot process ExpansionXVocab: Expansion dataset not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PIPELINE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nTimestamp: {timestamp}\")\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"  Core Dataset:          {len(df_core) if df_core is not None else 0:5d} patents\")\n",
    "print(f\"  Expansion Dataset:     {len(df_expansion) if df_expansion is not None else 0:5d} patents\")\n",
    "print(f\"  ExpansionXVocab:       {len(df_expansionxvocab) if 'df_expansionxvocab' in locals() and df_expansionxvocab is not None else 0:5d} patents\")\n",
    "\n",
    "print(f\"\\nCPC Code Coverage:\")\n",
    "print(f\"  Core CPC codes:        {len(CORE_CPC_CODES)}\")\n",
    "print(f\"  Expansion CPC codes:   {len(EXPANSION_CPC_CODES)}\")\n",
    "\n",
    "print(f\"\\nKeyword Filters:\")\n",
    "print(f\"  Keywords used:         {', '.join(VOCAB_KEYWORDS)}\")\n",
    "\n",
    "print(f\"\\nâœ“ Pipeline execution complete!\")\n",
    "print(f\"\\nAll data saved to: {BASE_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
