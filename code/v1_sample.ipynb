{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU Patent Data Pipeline v1\n",
    "\n",
    "This notebook fetches patent data from lens.org API based on GPU-related CPC codes and US jurisdiction.\n",
    "\n",
    "## Setup\n",
    "1. Install required packages: `pip install requests pandas pyarrow`\n",
    "2. Set your lens.org API token in environment variable `LENS_API_TOKEN`\n",
    "\n",
    "## Pipeline Steps\n",
    "1. Configure CPC codes and search parameters\n",
    "2. Fetch raw data from lens.org API\n",
    "3. Save raw compressed data\n",
    "4. Parse and normalize schema\n",
    "5. Clean text fields\n",
    "6. Generate embeddings (optional)\n",
    "7. Log all operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import gzip\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Configuration\n",
    "API_TOKEN = os.getenv('LENS_API_TOKEN', 'your-api-token-here')\n",
    "API_URL = 'https://api.lens.org/patent/search'\n",
    "\n",
    "# Data paths\n",
    "BASE_PATH = Path('../data/patents/v1_gpu_kw_cpc')\n",
    "RAW_PATH = BASE_PATH / 'raw'\n",
    "PARSED_PATH = BASE_PATH / 'parsed'\n",
    "TEXT_CLEAN_PATH = BASE_PATH / 'text_clean'\n",
    "EMBEDDINGS_PATH = BASE_PATH / 'embeddings'\n",
    "LOGS_PATH = BASE_PATH / 'logs'\n",
    "\n",
    "# GPU-related CPC codes (examples - adjust based on your research needs)\n",
    "# G06F = Electric digital data processing\n",
    "# G06T = Image data processing or generation\n",
    "# H01L = Semiconductor devices\n",
    "GPU_CPC_CODES = [\n",
    "    'G06F3/14',    # Graphics input/output\n",
    "    'G06T1/20',    # Parallel data processing\n",
    "    'G06T1/60',    # GPU architecture\n",
    "    'G09G5/36',    # Graphics processing\n",
    "]\n",
    "\n",
    "# Search parameters\n",
    "JURISDICTION = 'US'  # United States patents only\n",
    "MAX_RESULTS = 100    # Adjust based on your needs (lens.org has rate limits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Fetch Raw Data from lens.org API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_query(cpc_codes, jurisdiction):\n",
    "    \"\"\"\n",
    "    Build lens.org API query for GPU patents.\n",
    "    \n",
    "    Args:\n",
    "        cpc_codes: List of CPC classification codes\n",
    "        jurisdiction: Patent jurisdiction (e.g., 'US')\n",
    "    \n",
    "    Returns:\n",
    "        Query dictionary for lens.org API\n",
    "    \"\"\"\n",
    "    # Build CPC code filter\n",
    "    cpc_filter = {\n",
    "        \"bool\": {\n",
    "            \"should\": [\n",
    "                {\"term\": {\"classification_cpc.classification_id\": code}} \n",
    "                for code in cpc_codes\n",
    "            ],\n",
    "            \"minimum_should_match\": 1\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Add GPU-related keywords for better filtering\n",
    "    keyword_filter = {\n",
    "        \"bool\": {\n",
    "            \"should\": [\n",
    "                {\"match\": {\"title\": \"GPU\"}},\n",
    "                {\"match\": {\"title\": \"graphics processing\"}},\n",
    "                {\"match\": {\"title\": \"parallel processing\"}},\n",
    "                {\"match\": {\"abstract\": \"GPU\"}},\n",
    "                {\"match\": {\"abstract\": \"graphics processing unit\"}}\n",
    "            ],\n",
    "            \"minimum_should_match\": 1\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    query = {\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": [\n",
    "                    cpc_filter,\n",
    "                    keyword_filter,\n",
    "                    {\"term\": {\"jurisdiction\": jurisdiction}}\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        \"size\": MAX_RESULTS,\n",
    "        \"include\": [\n",
    "            \"lens_id\",\n",
    "            \"title\",\n",
    "            \"abstract\",\n",
    "            \"description\",\n",
    "            \"claims\",\n",
    "            \"date_published\",\n",
    "            \"jurisdiction\",\n",
    "            \"applicants\",\n",
    "            \"inventors\",\n",
    "            \"classification_cpc\",\n",
    "            \"biblio\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return query\n",
    "\n",
    "def fetch_patents(api_token, query):\n",
    "    \"\"\"\n",
    "    Fetch patents from lens.org API.\n",
    "    \n",
    "    Args:\n",
    "        api_token: lens.org API token\n",
    "        query: Query dictionary\n",
    "    \n",
    "    Returns:\n",
    "        API response data\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {api_token}',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    \n",
    "    response = requests.post(API_URL, json=query, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    return response.json()\n",
    "\n",
    "# Build and execute query\n",
    "query = build_query(GPU_CPC_CODES, JURISDICTION)\n",
    "print(\"Query configuration:\")\n",
    "print(json.dumps(query, indent=2))\n",
    "\n",
    "print(\"\\nFetching patents from lens.org...\")\n",
    "try:\n",
    "    raw_data = fetch_patents(API_TOKEN, query)\n",
    "    print(f\"✓ Successfully fetched {len(raw_data.get('data', []))} patents\")\n",
    "    print(f\"  Total results available: {raw_data.get('total', 0)}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error fetching data: {e}\")\n",
    "    raw_data = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Save Raw Compressed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if raw_data:\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    raw_file = RAW_PATH / f'patents_{timestamp}.json.gz'\n",
    "    \n",
    "    # Save compressed raw data\n",
    "    with gzip.open(raw_file, 'wt', encoding='utf-8') as f:\n",
    "        json.dump(raw_data, f, indent=2)\n",
    "    \n",
    "    print(f\"✓ Raw data saved to: {raw_file}\")\n",
    "    print(f\"  File size: {raw_file.stat().st_size / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Parse and Normalize Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_patents(raw_data):\n",
    "    \"\"\"\n",
    "    Parse raw patent data into normalized schema.\n",
    "    \n",
    "    Args:\n",
    "        raw_data: Raw API response\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with normalized patent data\n",
    "    \"\"\"\n",
    "    patents = raw_data.get('data', [])\n",
    "    \n",
    "    parsed_records = []\n",
    "    for patent in patents:\n",
    "        record = {\n",
    "            'lens_id': patent.get('lens_id'),\n",
    "            'title': patent.get('title'),\n",
    "            'abstract': patent.get('abstract'),\n",
    "            'description': patent.get('description'),\n",
    "            'date_published': patent.get('date_published'),\n",
    "            'jurisdiction': patent.get('jurisdiction'),\n",
    "            'applicants': json.dumps(patent.get('applicants', [])),\n",
    "            'inventors': json.dumps(patent.get('inventors', [])),\n",
    "            'cpc_codes': json.dumps([c.get('classification_id') for c in patent.get('classification_cpc', [])]),\n",
    "            'claims_count': len(patent.get('claims', [])),\n",
    "            'first_claim': patent.get('claims', [{}])[0].get('claim_text') if patent.get('claims') else None\n",
    "        }\n",
    "        parsed_records.append(record)\n",
    "    \n",
    "    return pd.DataFrame(parsed_records)\n",
    "\n",
    "if raw_data:\n",
    "    df_parsed = parse_patents(raw_data)\n",
    "    print(f\"✓ Parsed {len(df_parsed)} patent records\")\n",
    "    print(f\"\\nSchema:\")\n",
    "    print(df_parsed.dtypes)\n",
    "    print(f\"\\nSample record:\")\n",
    "    print(df_parsed.iloc[0] if len(df_parsed) > 0 else \"No records\")\n",
    "    \n",
    "    # Save as parquet\n",
    "    parsed_file = PARSED_PATH / f'patents_{timestamp}.parquet'\n",
    "    df_parsed.to_parquet(parsed_file, index=False)\n",
    "    print(f\"\\n✓ Parsed data saved to: {parsed_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clean Text Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean text field for embedding.\n",
    "    \n",
    "    Args:\n",
    "        text: Raw text\n",
    "    \n",
    "    Returns:\n",
    "        Cleaned text\n",
    "    \"\"\"\n",
    "    if not text or pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to string if not already\n",
    "    text = str(text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove special characters but keep basic punctuation\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s,.;:()-]', '', text)\n",
    "    \n",
    "    # Strip leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "if raw_data and not df_parsed.empty:\n",
    "    # Create cleaned version of key text fields\n",
    "    df_clean = pd.DataFrame({\n",
    "        'lens_id': df_parsed['lens_id'],\n",
    "        'title_clean': df_parsed['title'].apply(clean_text),\n",
    "        'abstract_clean': df_parsed['abstract'].apply(clean_text),\n",
    "        'description_clean': df_parsed['description'].apply(clean_text),\n",
    "        'first_claim_clean': df_parsed['first_claim'].apply(clean_text),\n",
    "        # Combined text for embedding\n",
    "        'combined_text': df_parsed.apply(\n",
    "            lambda row: f\"{clean_text(row['title'])} {clean_text(row['abstract'])} {clean_text(row['first_claim'])}\",\n",
    "            axis=1\n",
    "        )\n",
    "    })\n",
    "    \n",
    "    print(f\"✓ Cleaned {len(df_clean)} patent text records\")\n",
    "    print(f\"\\nSample cleaned record:\")\n",
    "    print(df_clean.iloc[0] if len(df_clean) > 0 else \"No records\")\n",
    "    \n",
    "    # Save cleaned text\n",
    "    clean_file = TEXT_CLEAN_PATH / f'patents_clean_{timestamp}.parquet'\n",
    "    df_clean.to_parquet(clean_file, index=False)\n",
    "    print(f\"\\n✓ Cleaned text saved to: {clean_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Embeddings (Optional)\n",
    "\n",
    "This section demonstrates how to generate embeddings. \n",
    "Requires a sentence transformer model (e.g., sentence-transformers library).\n",
    "\n",
    "```bash\n",
    "pip install sentence-transformers\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to generate embeddings\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# import numpy as np\n",
    "\n",
    "# if raw_data and not df_clean.empty:\n",
    "#     print(\"Loading embedding model...\")\n",
    "#     model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "#     \n",
    "#     print(\"Generating embeddings...\")\n",
    "#     embeddings = model.encode(df_clean['combined_text'].tolist(), show_progress_bar=True)\n",
    "#     \n",
    "#     # Save embeddings\n",
    "#     emb_file = EMBEDDINGS_PATH / f'embeddings_{timestamp}.npy'\n",
    "#     np.save(emb_file, embeddings)\n",
    "#     \n",
    "#     # Save IDs separately\n",
    "#     ids_file = EMBEDDINGS_PATH / f'ids_{timestamp}.txt'\n",
    "#     with open(ids_file, 'w') as f:\n",
    "#         f.write('\\n'.join(df_clean['lens_id'].tolist()))\n",
    "#     \n",
    "#     print(f\"✓ Embeddings saved to: {emb_file}\")\n",
    "#     print(f\"  Shape: {embeddings.shape}\")\n",
    "#     print(f\"✓ IDs saved to: {ids_file}\")\n",
    "\n",
    "print(\"Embedding generation is optional. Uncomment the code above to enable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Log Pipeline Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if raw_data:\n",
    "    # Create execution log\n",
    "    log = {\n",
    "        'timestamp': timestamp,\n",
    "        'execution_time': datetime.now().isoformat(),\n",
    "        'query_spec': query,\n",
    "        'results': {\n",
    "            'total_available': raw_data.get('total', 0),\n",
    "            'fetched': len(raw_data.get('data', [])),\n",
    "            'parsed': len(df_parsed) if raw_data else 0,\n",
    "            'cleaned': len(df_clean) if raw_data and not df_parsed.empty else 0\n",
    "        },\n",
    "        'files_created': {\n",
    "            'raw': str(raw_file.name) if raw_data else None,\n",
    "            'parsed': str(parsed_file.name) if raw_data else None,\n",
    "            'cleaned': str(clean_file.name) if raw_data and not df_parsed.empty else None\n",
    "        },\n",
    "        'cpc_codes': GPU_CPC_CODES,\n",
    "        'jurisdiction': JURISDICTION\n",
    "    }\n",
    "    \n",
    "    # Save log\n",
    "    log_file = LOGS_PATH / f'run_{timestamp}.json'\n",
    "    with open(log_file, 'w') as f:\n",
    "        json.dump(log, f, indent=2)\n",
    "    \n",
    "    print(f\"✓ Execution log saved to: {log_file}\")\n",
    "    print(f\"\\nPipeline Summary:\")\n",
    "    print(f\"  Total patents available: {log['results']['total_available']}\")\n",
    "    print(f\"  Fetched: {log['results']['fetched']}\")\n",
    "    print(f\"  Parsed: {log['results']['parsed']}\")\n",
    "    print(f\"  Cleaned: {log['results']['cleaned']}\")\n",
    "else:\n",
    "    print(\"⚠ No data fetched. Check your API token and connection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Analyze parsed data**: Load the parquet files and explore patent characteristics\n",
    "2. **Generate embeddings**: Uncomment section 5 to create vector embeddings\n",
    "3. **Similarity search**: Use embeddings to find similar patents\n",
    "4. **Time series analysis**: Analyze patent trends over time\n",
    "5. **Network analysis**: Explore relationships between applicants, inventors, and technologies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
